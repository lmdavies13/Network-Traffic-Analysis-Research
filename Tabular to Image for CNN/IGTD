{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyNiAwfoOM63UyKDa04bWOSs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Mount Drive"],"metadata":{"id":"Qo4I-rSsL538"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"iMs90nXCIHbQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load our Data"],"metadata":{"id":"jJGkZ-3JLeVv"}},{"cell_type":"code","source":["import pandas as pd\n","import re\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","\n","# Load our data paths\n","data_full_path = '/content/drive/My Drive/Colab Notebooks/Independent Study/Data/kddcup.data.gz'\n","test_path = '/content/drive/My Drive/Colab Notebooks/Independent Study/Data/corrected.gz'\n","names_path = '/content/drive/My Drive/Colab Notebooks/Independent Study/Data/kddcup.names'\n","\n","# Build our dataframe\n","data = pd.read_csv(data_full_path, compression='gzip', header=None)\n","testset = pd.read_csv(test_path, compression='gzip', header=None)\n","names = pd.read_csv(names_path, skiprows=1, header=None, sep=':')\n","data.columns = names.iloc[:,0].tolist() + ['label']\n","testset.columns = names.iloc[:,0].tolist() + ['label']\n","data['label'] = data['label'].apply(lambda x: 'normal' if x == 'normal.' else 'attack')\n","testset['label'] = testset['label'].apply(lambda x: 'normal' if x == 'normal.' else 'attack')\n","\n","print(\"unique labels in data:\", data['label'].unique())\n","print(\"unique labels in testset:\", testset['label'].unique())\n","\n","# declare our categorical values\n","categorical_one = ['protocol_type', 'service', 'flag']\n","categorical_two = ['land', 'logged_in', 'root_shell', 'su_attempted', 'is_guest_login', 'is_host_login']\n","cat_all = categorical_one + categorical_two\n","categorical_vars = cat_all\n","print(\"Categorical Columns Declared: \",categorical_vars)\n","\n","#Drop Duplicates on Training data\n","data = data.drop_duplicates()\n","#testset = testset.drop_duplicates()\n","new_shape = data.shape\n","constant_columns = [col for col in data.columns if data[col].nunique() == 1]\n","print(\"Constant Columns: \", constant_columns)\n","#if any of the categotical variables are found in constant_columns, remove from categorical_vars\n","for col in constant_columns:\n","    if col in categorical_vars:\n","        categorical_vars.remove(col)\n","\n","data = data.drop(columns=constant_columns)\n","testset = testset.drop(columns=constant_columns)\n","print(\"Columns Dropped: \", len(constant_columns))\n","print(\"New Categorical Columns: \", categorical_vars)\n","print(\"New Data Shape: \", data.shape)\n","print(\"New Test Set Shape: \", testset.shape)"],"metadata":{"id":"EdsWOlzxaScf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(data['count'].unique())"],"metadata":{"id":"rlwzyKeS5-ZR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import MinMaxScaler\n","\n","'''\n","#take on 10% of the training set and the test set\n","data = data.sample(frac=0.1, random_state=42)\n","testset = testset.sample(frac=0.1, random_state=42)\n","'''\n","\n","X_train = data.drop(columns=['label'])\n","X_test = testset.drop(columns=['label'])\n","y_train = data['label']\n","y_test = testset['label']\n","\n","print('\\n\\n\\n')\n","print(\"X_train Shape: \", X_train.shape)\n","print(\"X_test Shape: \", X_test.shape)\n","print(\"y_train Shape: \", y_train.shape)\n","print(\"y_test Shape: \", y_test.shape)\n","\n","embeddings_train = X_train.copy()\n","embeddings_test = X_test.copy()\n","'''\n","#these columns we convert to boolean\n","columns_to_check = ['land', 'logged_in', 'is_guest_login', 'is_host_login']\n","for col in columns_to_check:\n","  embeddings_train[col] = embeddings_train[col].astype(bool)\n","  embeddings_test[col] = embeddings_test[col].astype(bool)\n","'''\n","\n","'''\n","# Calculate mean and std from training set\n","mean = embeddings_train.select_dtypes(include=[np.number]).mean()\n","std = embeddings_train.select_dtypes(include=[np.number]).std()\n","\n","# Normalize numeric columns using training mean and std\n","def normalize_numeric_columns(df, mean, std):\n","    numeric_columns = df.select_dtypes(include=[np.number]).columns\n","    df[numeric_columns] = (df[numeric_columns] - mean) / std\n","    return df\n","\n","X_train = normalize_numeric_columns(embeddings_train, mean, std)\n","X_test = normalize_numeric_columns(embeddings_test, mean, std)\n","'''\n","\n","\n","# Ensure that indices are aligned before concatenation\n","X_train.reset_index(drop=True, inplace=True)\n","X_test.reset_index(drop=True, inplace=True)\n","y_train.reset_index(drop=True, inplace=True)\n","y_test.reset_index(drop=True, inplace=True)\n","\n","\n","\n","# Map labels to integers\n","tag_mapping = {'normal': 0, 'attack': 1}\n","y_train = y_train.map(tag_mapping)\n","y_test = y_test.map(tag_mapping)\n","\n","#onehot and normalize\n","import pandas as pd\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","\n","def label_encode(X_train, X_test, categorical_vars):\n","    # Label encode categorical variables\n","    le = LabelEncoder()\n","    for col in X_train.columns:\n","        if col in categorical_vars:\n","            X_train[col] = le.fit_transform(X_train[col])\n","            X_test[col] = le.fit_transform(X_test[col])\n","    return X_train, X_test\n","\n","def one_hot_encode(X_train, X_test, categorical_vars):\n","    # One-hot encode categorical variables\n","    X_train = pd.get_dummies(X_train, columns=categorical_vars).astype(int)\n","    X_test = pd.get_dummies(X_test, columns=categorical_vars).astype(int)\n","\n","    # Align the columns of the test set to match the train set\n","    # Add any new columns in the test set to the train set with values of 0\n","    for col in X_test.columns:\n","        if col not in X_train.columns:\n","            X_train[col] = 0\n","\n","    # Reindex the test set columns to match the train set columns\n","    X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n","\n","    return X_train, X_test\n","\n","\n","# make numeric_cols the rest of the columns that dont appear in categorical_vars\n","numeric_cols = [col for col in X_train.columns if col not in categorical_vars]\n","print(\"Numeric Columns: \", numeric_cols)\n","def standardize(X_train, X_test):\n","    # Standardize numerical variables\n","    scaler = MinMaxScaler()\n","    X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n","    X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])\n","    return X_train, X_test\n","\n","def apply_pca(X_train, X_test, variance_threshold=0.85):\n","    pca = PCA()\n","    pca.fit(X_train)\n","    cumsum = np.cumsum(pca.explained_variance_ratio_)\n","    n_components = np.where(cumsum >= variance_threshold)[0][0] + 1\n","    pca = PCA(n_components=n_components)\n","    X_train_pca = pca.fit_transform(X_train)\n","    X_test_pca = pca.transform(X_test)\n","\n","    # Convert back to DataFrame\n","    X_train_pca_df = pd.DataFrame(X_train_pca, index=X_train.index)\n","    X_test_pca_df = pd.DataFrame(X_test_pca, index=X_test.index)\n","\n","    return X_train_pca_df, X_test_pca_df, n_components\n","\n","\n","# one hot encode the categorical variables\n","X_train, X_test = one_hot_encode(X_train, X_test, categorical_one)\n","\n","print(\"Post process shapes:\")\n","print(\"X_train Shape: \", X_train.shape)\n","print(\"X_test Shape: \", X_test.shape)\n","print(\"y_train Shape: \", y_train.shape)\n","print(\"y_test Shape: \", y_test.shape)\n","\n","\n","# Convert y labels to numerical format\n","y_train_ml = y_train.map({'normal': 0, 'anomaly': 1})\n","y_test_ml = y_test.map({'normal': 0, 'anomaly': 1})\n","\n","print(X_train.shape)\n","print(X_test.shape)\n","print(y_train.shape)\n","print(y_test.shape)\n"],"metadata":{"id":"Z3DYjF1zHp2g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(20, 15))\n","for i, column in enumerate(numeric_cols, 1):\n","    plt.subplot((len(numeric_cols) + 4) // 5, 5, i)  # Adjust rows and columns based on number of features\n","    sns.histplot(X_train[column], kde=True, bins=20)\n","    plt.title(f'Normalized Histogram of {column}')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"uax_yrtH6oIC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train.head(100)"],"metadata":{"id":"FYs3CMKhjCnY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","\n","\n","# Histograms\n","plt.figure(figsize=(12, 8))\n","for i, column in enumerate(numeric_cols, 1):\n","    plt.subplot((len(numeric_cols) + 2) // 3, 3, i)\n","    sns.histplot(X_train[column], kde=True, bins=20)\n","    plt.title(f'Histogram of {column}')\n","plt.tight_layout()\n","plt.show()\n","\n","# Box plots\n","plt.figure(figsize=(12, 8))\n","sns.boxplot(data=X_train[numeric_cols])\n","plt.title('Box plots of Normalized Features')\n","plt.xticks(rotation=90)\n","plt.show()\n"],"metadata":{"id":"VUvNXV_k3QKj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_train['count'].unique())"],"metadata":{"id":"wb5f_tMQ5Osa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot histograms for each normalized feature\n","plt.figure(figsize=(20, 15))\n","for i, column in enumerate(numeric_cols, 1):\n","    plt.subplot((len(numeric_cols) + 4) // 5, 5, i)  # Adjust rows and columns based on number of features\n","    sns.histplot(X_train[column], kde=True, bins=20)\n","    plt.title(f'Histogram of {column}')\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"JbbZ60674OrV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tabular to Image Code"],"metadata":{"id":"_zo3tLs1Lh-4"}},{"cell_type":"code","source":["from scipy.stats import spearmanr, rankdata\n","from scipy.spatial.distance import pdist, squareform\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import pandas as pd\n","import shutil\n","import time\n","import _pickle as cp\n","import sys\n","from astropy.stats import median_absolute_deviation\n","\n","\n","def select_features_by_variation(data, variation_measure='var', threshold=None, num=None, draw_histogram=False,\n","                                 bins=100, log=False):\n","    '''\n","    This function evaluates the variations of individual features and returns the indices of features with large\n","    variations. Missing values are ignored in evaluating variation.\n","\n","    Parameters:\n","    -----------\n","    data: numpy array or pandas data frame of numeric values, with a shape of [n_samples, n_features].\n","    variation_metric: string indicating the metric used for evaluating feature variation. 'var' indicates variance;\n","        'std' indicates standard deviation; 'mad' indicates median absolute deviation. Default is 'var'.\n","    threshold: float. Features with a variation larger than threshold will be selected. Default is None.\n","    num: positive integer. It is the number of features to be selected based on variation.\n","        The number of selected features will be the smaller of num and the total number of\n","        features with non-missing variations. Default is None. threshold and portion can not take values\n","        and be used simultaneously.\n","    draw_histogram: boolean, whether to draw a histogram of feature variations. Default is False.\n","    bins: positive integer, the number of bins in the histogram. Default is the smaller of 50 and the number of\n","        features with non-missing variations.\n","    log: boolean, indicating whether the histogram should be drawn on log scale.\n","\n","\n","    Returns:\n","    --------\n","    indices: 1-D numpy array containing the indices of selected features. If both threshold and\n","        portion are None, indices will be an empty array.\n","    '''\n","\n","    if isinstance(data, pd.DataFrame):\n","        data = data.values\n","    elif not isinstance(data, np.ndarray):\n","        print('Input data must be a numpy array or pandas data frame')\n","        sys.exit(1)\n","\n","    if variation_measure == 'std':\n","        v_all = np.nanstd(a=data, axis=0)\n","    elif variation_measure == 'mad':\n","        v_all = median_absolute_deviation(data=data, axis=0, ignore_nan=True)\n","    else:\n","        v_all = np.nanvar(a=data, axis=0)\n","\n","    indices = np.where(np.invert(np.isnan(v_all)))[0]\n","    v = v_all[indices]\n","\n","    if draw_histogram:\n","        if len(v) < 50:\n","            print('There must be at least 50 features with variation measures to draw a histogram')\n","        else:\n","            bins = int(min(bins, len(v)))\n","            _ = plt.hist(v, bins=bins, log=log)\n","            plt.show()\n","\n","    if threshold is None and num is None:\n","        return np.array([])\n","    elif threshold is not None and num is not None:\n","        print('threshold and portion can not be used simultaneously. Only one of them can take a real value')\n","        sys.exit(1)\n","\n","    if threshold is not None:\n","        indices = indices[np.where(v > threshold)[0]]\n","    else:\n","        n_f = int(min(num, len(v)))\n","        indices = indices[np.argsort(-v)[:n_f]]\n","\n","    indices = np.sort(indices)\n","\n","    return indices\n","\n","\n","\n","def min_max_transform(data):\n","    '''\n","    This function does a linear transformation of each feature, so that the minimum and maximum values of a\n","    feature are 0 and 1, respectively.\n","\n","    Input:\n","    data: an input data array with a size of [n_sample, n_feature]\n","    Return:\n","    norm_data: the data array after transformation\n","    '''\n","\n","    norm_data = np.empty(data.shape)\n","    norm_data.fill(np.nan)\n","    for i in range(data.shape[1]):\n","        v = data[:, i].copy()\n","        if np.max(v) == np.min(v):\n","            norm_data[:, i] = 0\n","        else:\n","            v = (v - np.min(v)) / (np.max(v) - np.min(v))\n","            norm_data[:, i] = v\n","    return norm_data\n","\n","\n","\n","def generate_feature_distance_ranking(data, method='Pearson'):\n","    '''\n","    This function generates ranking of distances/dissimilarities between features for tabular data.\n","\n","    Input:\n","    data: input data, n_sample by n_feature\n","    method: 'Euclidean' calculates similarity between features based on Euclidean distance;\n","        'Pearson' uses Pearson correlation coefficient to evaluate similarity between features;\n","        'Spearman' uses Spearman correlation coefficient to evaluate similarity between features;\n","        'set' uses Jaccard index to evaluate similarity between features that are binary variables.\n","\n","    Return:\n","    ranking: symmetric ranking matrix based on dissimilarity\n","    corr: matrix of distances between features\n","    '''\n","\n","    num = data.shape[1]\n","    if method == 'Pearson':\n","        corr = np.corrcoef(np.transpose(data))\n","    elif method == 'Spearman':\n","        corr = spearmanr(data).correlation\n","    elif method == 'Euclidean':\n","        corr = squareform(pdist(np.transpose(data), metric='euclidean'))\n","        corr = np.max(corr) - corr\n","        corr = corr / np.max(corr)\n","    elif method == 'set':  # This is the new set operation to calculate similarity. It does not tolerate all-zero features.\n","        corr1 = np.dot(np.transpose(data), data)\n","        corr2 = data.shape[0] - np.dot(np.transpose(1 - data), 1 - data)\n","        corr = corr1 / corr2\n","\n","    corr = 1 - corr\n","    corr = np.around(a=corr, decimals=10)\n","\n","    tril_id = np.tril_indices(num, k=-1)\n","    rank = rankdata(corr[tril_id])\n","    ranking = np.zeros((num, num))\n","    ranking[tril_id] = rank\n","    ranking = ranking + np.transpose(ranking)\n","\n","    return ranking, corr\n","\n","\n","\n","def generate_matrix_distance_ranking(num_r, num_c, method='Euclidean', num=None):\n","    '''\n","    This function calculates the ranking of distances between all pairs of entries in a matrix of size num_r by num_c.\n","\n","    Input:\n","    num_r: number of rows in the matrix\n","    num_c: number of columns in the matrix\n","    method: method used to calculate distance. Can be 'Euclidean' or 'Manhattan'.\n","    num: number of real features. If None, num = num_r * num_c. If num < num_r * num_c, num_r * num_c - num\n","        zeros will be padded to the image representation.\n","\n","    Return:\n","    coordinate: a num-by-2 matrix giving the coordinates of elements in the matrix.\n","    ranking: a num-by-num matrix giving the ranking of pair-wise distance.\n","\n","    '''\n","\n","    if num is None:\n","        num = num_r * num_c\n","\n","    # generate the coordinates of elements in a matrix\n","    for r in range(num_r):\n","        if r == 0:\n","            coordinate = np.transpose(np.vstack((np.zeros(num_c), range(num_c))))\n","        else:\n","            coordinate = np.vstack((coordinate, np.transpose(np.vstack((np.ones(num_c) * r, range(num_c))))))\n","    coordinate = coordinate[:num, :]\n","\n","    # calculate the closeness of the elements\n","    cord_dist = np.zeros((num, num))\n","    if method == 'Euclidean':\n","        for i in range(num):\n","            cord_dist[i, :] = np.sqrt(np.square(coordinate[i, 0] * np.ones(num) - coordinate[:, 0]) +\n","                                     np.square(coordinate[i, 1] * np.ones(num) - coordinate[:, 1]))\n","    elif method == 'Manhattan':\n","        for i in range(num):\n","            cord_dist[i, :] = np.abs(coordinate[i, 0] * np.ones(num) - coordinate[:, 0]) + \\\n","                             np.abs(coordinate[i, 1] * np.ones(num) - coordinate[:, 1])\n","\n","    # generate the ranking based on distance\n","    tril_id = np.tril_indices(num, k=-1)\n","    rank = rankdata(cord_dist[tril_id])\n","    ranking = np.zeros((num, num))\n","    ranking[tril_id] = rank\n","    ranking = ranking + np.transpose(ranking)\n","\n","    coordinate = np.int64(coordinate)\n","    return (coordinate[:, 0], coordinate[:, 1]), ranking\n","\n","\n","\n","def IGTD_absolute_error(source, target, max_step=1000, switch_t=0, val_step=50, min_gain=0.00001, random_state=1,\n","                        save_folder=None, file_name=''):\n","    '''\n","    This function switches the order of rows (columns) in the source ranking matrix to make it similar to the target\n","    ranking matrix. In each step, the algorithm randomly picks a row that has not been switched with others for\n","    the longest time and checks all possible switch of this row, and selects the switch that reduces the\n","    dissimilarity most. Dissimilarity (i.e. the error) is the summation of absolute difference of\n","    lower triangular elements between the rearranged source ranking matrix and the target ranking matrix.\n","\n","    Input:\n","    source: a symmetric ranking matrix with zero diagonal elements.\n","    target: a symmetric ranking matrix with zero diagonal elements. 'source' and 'target' should have the same size.\n","    max_step: the maximum steps that the algorithm should run if never converges.\n","    switch_t: the threshold to determine whether feature switching should happen\n","    val_step: number of steps for checking gain on the objective function to determine convergence\n","    min_gain: if the objective function is not improved more than 'min_gain' in 'val_step' steps,\n","        the algorithm terminates.\n","    random_state: for setting random seed.\n","    save_folder: a path to save the picture of source ranking matrix in the optimization process.\n","    file_name: a string as part of the file names for saving results\n","\n","    Return:\n","    index_record: indices to rearrange the rows(columns) in source obtained the optimization process\n","    err_record: error obtained in the optimization process\n","    run_time: the time at which each step is completed in the optimization process\n","    '''\n","\n","    np.random.RandomState(seed=random_state)\n","    if os.path.exists(save_folder):\n","        shutil.rmtree(save_folder)\n","    os.mkdir(save_folder)\n","\n","    source = source.copy()\n","    num = source.shape[0]\n","    tril_id = np.tril_indices(num, k=-1)\n","    index = np.array(range(num))\n","    index_record = np.empty((max_step + 1, num))\n","    index_record.fill(np.nan)\n","    index_record[0, :] = index.copy()\n","\n","    # calculate the error associated with each row\n","    err_v = np.empty(num)\n","    err_v.fill(np.nan)\n","    for i in range(num):\n","        err_v[i] = np.sum(np.abs(source[i, 0:i] - target[i, 0:i])) + \\\n","                   np.sum(np.abs(source[(i + 1):, i] - target[(i + 1):, i]))\n","\n","    step_record = -np.ones(num)\n","    err_record = [np.sum(abs(source[tril_id] - target[tril_id]))]\n","    pre_err = err_record[0]\n","    t1 = time.time()\n","    run_time = [0]\n","\n","    for s in range(max_step):\n","        delta = - np.ones(num) * np.inf\n","\n","        # randomly pick a row that has not been considered for the longest time\n","        idr = np.where(step_record == np.min(step_record))[0]\n","        ii = idr[np.random.permutation(len(idr))[0]]\n","\n","        for jj in range(num):\n","            if jj == ii:\n","                continue\n","\n","            if ii < jj:\n","                i = ii\n","                j = jj\n","            else:\n","                i = jj\n","                j = ii\n","\n","            err_ori = err_v[i] + err_v[j] - np.abs(source[j, i] - target[j, i])\n","\n","            err_i = np.sum(np.abs(source[j, :i] - target[i, :i])) + \\\n","                    np.sum(np.abs(source[(i + 1):j, j] - target[(i + 1):j, i])) + \\\n","                    np.sum(np.abs(source[(j + 1):, j] - target[(j + 1):, i])) + np.abs(source[i, j] - target[j, i])\n","            err_j = np.sum(np.abs(source[i, :i] - target[j, :i])) + \\\n","                    np.sum(np.abs(source[i, (i + 1):j] - target[j, (i + 1):j])) + \\\n","                    np.sum(np.abs(source[(j + 1):, i] - target[(j + 1):, j])) + np.abs(source[i, j] - target[j, i])\n","            err_test = err_i + err_j - np.abs(source[i, j] - target[j, i])\n","\n","            delta[jj] = err_ori - err_test\n","\n","        delta_norm = delta / pre_err\n","        id = np.where(delta_norm >= switch_t)[0]\n","        if len(id) > 0:\n","            jj = np.argmax(delta)\n","\n","            # Update the error associated with each row\n","            if ii < jj:\n","                i = ii\n","                j = jj\n","            else:\n","                i = jj\n","                j = ii\n","            for k in range(num):\n","                if k < i:\n","                    err_v[k] = err_v[k] - np.abs(source[i, k] - target[i, k]) - np.abs(source[j, k] - target[j, k]) + \\\n","                               np.abs(source[j, k] - target[i, k]) + np.abs(source[i, k] - target[j, k])\n","                elif k == i:\n","                    err_v[k] = np.sum(np.abs(source[j, :i] - target[i, :i])) + \\\n","                    np.sum(np.abs(source[(i + 1):j, j] - target[(i + 1):j, i])) + \\\n","                    np.sum(np.abs(source[(j + 1):, j] - target[(j + 1):, i])) + np.abs(source[i, j] - target[j, i])\n","                elif k < j:\n","                    err_v[k] = err_v[k] - np.abs(source[k, i] - target[k, i]) - np.abs(source[j, k] - target[j, k]) + \\\n","                               np.abs(source[k, j] - target[k, i]) + np.abs(source[i, k] - target[j, k])\n","                elif k == j:\n","                    err_v[k] = np.sum(np.abs(source[i, :i] - target[j, :i])) + \\\n","                    np.sum(np.abs(source[i, (i + 1):j] - target[j, (i + 1):j])) + \\\n","                    np.sum(np.abs(source[(j + 1):, i] - target[(j + 1):, j])) + np.abs(source[i, j] - target[j, i])\n","                else:\n","                    err_v[k] = err_v[k] - np.abs(source[k, i] - target[k, i]) - np.abs(source[k, j] - target[k, j]) + \\\n","                               np.abs(source[k, j] - target[k, i]) + np.abs(source[k, i] - target[k, j])\n","\n","            # switch rows i and j\n","            ii_v = source[ii, :].copy()\n","            jj_v = source[jj, :].copy()\n","            source[ii, :] = jj_v\n","            source[jj, :] = ii_v\n","            ii_v = source[:, ii].copy()\n","            jj_v = source[:, jj].copy()\n","            source[:, ii] = jj_v\n","            source[:, jj] = ii_v\n","            err = pre_err - delta[jj]\n","\n","            # update rearrange index\n","            t = index[ii]\n","            index[ii] = index[jj]\n","            index[jj] = t\n","\n","            # update step record\n","            step_record[ii] = s\n","            step_record[jj] = s\n","        else:\n","            # error is not changed due to no switch\n","            err = pre_err\n","\n","            # update step record\n","            step_record[ii] = s\n","\n","        err_record.append(err)\n","        print('Step ' + str(s) + ' err: ' + str(err))\n","        index_record[s + 1, :] = index.copy()\n","        run_time.append(time.time() - t1)\n","\n","        if s > val_step:\n","            if np.sum((err_record[-val_step - 1] - np.array(err_record[(-val_step):])) / err_record[\n","                -val_step - 1] >= min_gain) == 0:\n","                break\n","\n","        pre_err = err\n","\n","    index_record = index_record[:len(err_record), :].astype(np.int64)\n","    if save_folder is not None:\n","        pd.DataFrame(index_record).to_csv(save_folder + '/' + file_name + '_index.txt', header=False, index=False,\n","            sep='\\t', lineterminator='\\r\\n')\n","        pd.DataFrame(np.transpose(np.vstack((err_record, np.array(range(s + 2))))),\n","            columns=['error', 'steps']).to_csv(save_folder + '/' + file_name + '_error_and_step.txt',\n","            header=True, index=False, sep='\\t', lineterminator='\\r\\n')\n","        pd.DataFrame(np.transpose(np.vstack((err_record, run_time))), columns=['error', 'run_time']).to_csv(\n","            save_folder + '/' + file_name + '_error_and_time.txt', header=True, index=False, sep='\\t',\n","            lineterminator='\\r\\n')\n","\n","    return index_record, err_record, run_time\n","\n","\n","\n","def IGTD_square_error(source, target, max_step=1000, switch_t=0, val_step=50, min_gain=0.00001, random_state=1,\n","                      save_folder=None, file_name=''):\n","    '''\n","    This function switches the order of rows (columns) in the source ranking matrix to make it similar to the target\n","    ranking matrix. In each step, the algorithm randomly picks a row that has not been switched with others for\n","    the longest time and checks all possible switch of this row, and selects the switch that reduces the\n","    dissimilarity most. Dissimilarity (i.e. the error) is the summation of squared difference of\n","    lower triangular elements between the rearranged source ranking matrix and the target ranking matrix.\n","\n","    Input:\n","    source: a symmetric ranking matrix with zero diagonal elements.\n","    target: a symmetric ranking matrix with zero diagonal elements. 'source' and 'target' should have the same size.\n","    max_step: the maximum steps that the algorithm should run if never converges.\n","    switch_t: the threshold to determine whether feature switching should happen\n","    val_step: number of steps for checking gain on the objective function to determine convergence\n","    min_gain: if the objective function is not improved more than 'min_gain' in 'val_step' steps,\n","        the algorithm terminates.\n","    random_state: for setting random seed.\n","    save_folder: a path to save the picture of source ranking matrix in the optimization process.\n","    file_name: a string as part of the file names for saving results\n","\n","    Return:\n","    index_record: ordering index to rearrange the rows(columns) in 'source' in the optimization process\n","    err_record: the error history in the optimization process\n","    run_time: the time at which each step is finished in the optimization process\n","    '''\n","\n","\n","    np.random.RandomState(seed=random_state)\n","    if os.path.exists(save_folder):\n","        shutil.rmtree(save_folder)\n","    os.mkdir(save_folder)\n","\n","    source = source.copy()\n","    num = source.shape[0]\n","    tril_id = np.tril_indices(num, k=-1)\n","    index = np.array(range(num))\n","    index_record = np.empty((max_step + 1, num))\n","    index_record.fill(np.nan)\n","    index_record[0, :] = index.copy()\n","\n","    # calculate the error associated with each row\n","    err_v = np.empty(num)\n","    err_v.fill(np.nan)\n","    for i in range(num):\n","        err_v[i] = np.sum(np.square(source[i, 0:i] - target[i, 0:i])) + \\\n","                   np.sum(np.square(source[(i + 1):, i] - target[(i + 1):, i]))\n","\n","    step_record = -np.ones(num)\n","    err_record = [np.sum(np.square(source[tril_id] - target[tril_id]))]\n","    pre_err = err_record[0]\n","    t1 = time.time()\n","    run_time = [0]\n","\n","    for s in range(max_step):\n","        delta = - np.ones(num) * np.inf\n","\n","        # randomly pick a row that has not been considered for the longest time\n","        idr = np.where(step_record == np.min(step_record))[0]\n","        ii = idr[np.random.permutation(len(idr))[0]]\n","\n","        for jj in range(num):\n","            if jj == ii:\n","                continue\n","\n","            if ii < jj:\n","                i = ii\n","                j = jj\n","            else:\n","                i = jj\n","                j = ii\n","\n","            err_ori = err_v[i] + err_v[j] - np.square(source[j, i] - target[j, i])\n","\n","            err_i = np.sum(np.square(source[j, :i] - target[i, :i])) + \\\n","                    np.sum(np.square(source[(i + 1):j, j] - target[(i + 1):j, i])) + \\\n","                    np.sum(np.square(source[(j + 1):, j] - target[(j + 1):, i])) + np.square(source[i, j] - target[j, i])\n","            err_j = np.sum(np.square(source[i, :i] - target[j, :i])) + \\\n","                    np.sum(np.square(source[i, (i + 1):j] - target[j, (i + 1):j])) + \\\n","                    np.sum(np.square(source[(j + 1):, i] - target[(j + 1):, j])) + np.square(source[i, j] - target[j, i])\n","            err_test = err_i + err_j - np.square(source[i, j] - target[j, i])\n","\n","            delta[jj] = err_ori - err_test\n","\n","        delta_norm = delta / pre_err\n","        id = np.where(delta_norm >= switch_t)[0]\n","        if len(id) > 0:\n","            jj = np.argmax(delta)\n","\n","            # Update the error associated with each row\n","            if ii < jj:\n","                i = ii\n","                j = jj\n","            else:\n","                i = jj\n","                j = ii\n","            for k in range(num):\n","                if k < i:\n","                    err_v[k] = err_v[k] - np.square(source[i, k] - target[i, k]) - np.square(source[j, k] - target[j, k]) + \\\n","                               np.square(source[j, k] - target[i, k]) + np.square(source[i, k] - target[j, k])\n","                elif k == i:\n","                    err_v[k] = np.sum(np.square(source[j, :i] - target[i, :i])) + \\\n","                        np.sum(np.square(source[(i + 1):j, j] - target[(i + 1):j, i])) + \\\n","                        np.sum(np.square(source[(j + 1):, j] - target[(j + 1):, i])) + np.square(source[i, j] - target[j, i])\n","                elif k < j:\n","                    err_v[k] = err_v[k] - np.square(source[k, i] - target[k, i]) - np.square(source[j, k] - target[j, k]) + \\\n","                               np.square(source[k, j] - target[k, i]) + np.square(source[i, k] - target[j, k])\n","                elif k == j:\n","                    err_v[k] = np.sum(np.square(source[i, :i] - target[j, :i])) + \\\n","                        np.sum(np.square(source[i, (i + 1):j] - target[j, (i + 1):j])) + \\\n","                        np.sum(np.square(source[(j + 1):, i] - target[(j + 1):, j])) + np.square(source[i, j] - target[j, i])\n","                else:\n","                    err_v[k] = err_v[k] - np.square(source[k, i] - target[k, i]) - np.square(source[k, j] - target[k, j]) + \\\n","                               np.square(source[k, j] - target[k, i]) + np.square(source[k, i] - target[k, j])\n","\n","            # switch rows i and j\n","            ii_v = source[ii, :].copy()\n","            jj_v = source[jj, :].copy()\n","            source[ii, :] = jj_v\n","            source[jj, :] = ii_v\n","            ii_v = source[:, ii].copy()\n","            jj_v = source[:, jj].copy()\n","            source[:, ii] = jj_v\n","            source[:, jj] = ii_v\n","            err = pre_err - delta[jj]\n","\n","            # update rearrange index\n","            t = index[ii]\n","            index[ii] = index[jj]\n","            index[jj] = t\n","\n","            # update step record\n","            step_record[ii] = s\n","            step_record[jj] = s\n","        else:\n","            # error is not changed due to no switch\n","            err = pre_err\n","\n","            # update step record\n","            step_record[ii] = s\n","\n","        err_record.append(err)\n","        print('Step ' + str(s) + ' err: ' + str(err))\n","        index_record[s + 1, :] = index.copy()\n","        run_time.append(time.time() - t1)\n","\n","        if s > val_step:\n","            if np.sum((err_record[-val_step - 1] - np.array(err_record[(-val_step):])) / err_record[\n","                -val_step - 1] >= min_gain) == 0:\n","                break\n","\n","        pre_err = err\n","\n","    index_record = index_record[:len(err_record), :].astype(np.int64)\n","    if save_folder is not None:\n","        pd.DataFrame(index_record).to_csv(save_folder + '/' + file_name + '_index.txt', header=False, index=False,\n","            sep='\\t', lineterminator='\\r\\n')\n","        pd.DataFrame(np.transpose(np.vstack((err_record, np.array(range(s + 2))))),\n","            columns=['error', 'steps']).to_csv(save_folder + '/' + file_name + '_error_and_step.txt',\n","            header=True, index=False, sep='\\t', lineterminator='\\r\\n')\n","        pd.DataFrame(np.transpose(np.vstack((err_record, run_time))), columns=['error', 'run_time']).to_csv(\n","            save_folder + '/' + file_name + '_error_and_time.txt', header=True, index=False, sep='\\t',\n","            lineterminator='\\r\\n')\n","\n","    return index_record, err_record, run_time\n","\n","\n","\n","def IGTD(source, target, err_measure='abs', max_step=1000, switch_t=0, val_step=50, min_gain=0.00001, random_state=1,\n","         save_folder=None, file_name=''):\n","    '''\n","    This is just a wrapper function that wraps the two search functions using different error measures.\n","    '''\n","\n","    if err_measure == 'abs':\n","        index_record, err_record, run_time = IGTD_absolute_error(source=source,\n","            target=target, max_step=max_step, switch_t=switch_t, val_step=val_step, min_gain=min_gain,\n","            random_state=random_state, save_folder=save_folder, file_name=file_name)\n","    if err_measure == 'squared':\n","        index_record, err_record, run_time = IGTD_square_error(source=source,\n","            target=target, max_step=max_step, switch_t=switch_t, val_step=val_step, min_gain=min_gain,\n","            random_state=random_state, save_folder=save_folder, file_name=file_name)\n","\n","    return index_record, err_record, run_time\n","\n","\n","\n","def generate_image_data(data, index, num_row, num_column, coord, image_folder=None, file_name=''):\n","    '''\n","    This function generates the data in image format according to rearrangement indices. It saves the data\n","    sample-by-sample in both txt files and image files\n","\n","    Input:\n","    data: original tabular data, 2D array or data frame, n_samples by n_features\n","    index: indices of features obtained through optimization, according to which the features can be\n","        arranged into a num_r by num_c image.\n","    num_row: number of rows in image\n","    num_column: number of columns in image\n","    coord: coordinates of features in the image/matrix\n","    image_folder: directory to save the image and txt data files. If none, no data file is saved\n","    file_name: a string as a part of the file names to save data\n","\n","    Return:\n","    image_data: the generated data, a 3D numpy array. The third dimension is across samples. The range of values\n","        is [0, 255]. Small values actually indicate high values in the original data.\n","    samples: the names of indices of the samples\n","    '''\n","\n","    if isinstance(data, pd.DataFrame):\n","        samples = data.index.map(str)\n","        data = data.values\n","    else:\n","        samples = [str(i) for i in range(data.shape[0])]\n","\n","    if os.path.exists(image_folder):\n","        shutil.rmtree(image_folder)\n","    os.mkdir(image_folder)\n","\n","    data_2 = data.copy()\n","    data_2 = data_2[:, index]\n","    max_v = np.max(data_2)\n","    min_v = np.min(data_2)\n","    data_2 = 255 - (data_2 - min_v) / (max_v - min_v) * 255 # Black color in heatmap indicates high value\n","\n","    image_data = np.empty((num_row, num_column, data_2.shape[0]))\n","    image_data.fill(np.nan)\n","    for i in range(data_2.shape[0]):\n","        data_i = np.empty((num_row, num_column))\n","        data_i.fill(np.nan)\n","        data_i[coord] = data_2[i, :]\n","\n","        # find nan in data_i and change them to 255\n","        idd = np.where(np.isnan(data_i))\n","        data_i[idd] = 255\n","\n","        image_data[:, :, i] = data_i\n","        image_data[:, :, i] = 255 - image_data[:, :, i] # High values in the array format of image data correspond\n","                                                        # to high values in tabular data\n","        if image_folder is not None:\n","            fig = plt.figure()\n","            plt.imshow(data_i, cmap='gray', vmin=0, vmax=255)\n","            plt.axis('scaled')\n","            plt.savefig(fname=image_folder + '/' + file_name + '_' + samples[i] + '_image.png', bbox_inches='tight',\n","                        pad_inches=0)\n","            plt.close(fig)\n","\n","            pd.DataFrame(image_data[:, :, i], index=None, columns=None).to_csv(image_folder + '/' + file_name + '_'\n","                + samples[i] + '_data.txt', header=None, index=None, sep='\\t', lineterminator='\\r\\n')\n","\n","    return image_data, samples\n","\n","\n","\n","def table_to_image(norm_d, scale, fea_dist_method, image_dist_method, save_image_size, max_step, val_step, normDir,\n","                   error, switch_t=0, min_gain=0.00001):\n","    '''\n","    This function converts tabular data into images using the IGTD algorithm.\n","\n","    Input:\n","    norm_d: a 2D array or data frame, which is the tabular data. Its size is n_samples by n_features\n","    scale: a list of two positive integers. It includes the numbers of pixel rows and columns in the image\n","        representation. The total number of pixels should not be smaller than the number of features,\n","        i.e. scale[0] * scale[1] >= n_features.\n","    fea_dist_method: a string indicating the method used for calculating the pairwise distances between features,\n","        for which there are three options.\n","        'Pearson' uses the Pearson correlation coefficient to evaluate the similarity between features.\n","        'Spearman' uses the Spearman correlation coefficient to evaluate the similarity between features.\n","        'set' uses the Jaccard index to evaluate the similarity between features that are binary variables.\n","    image_dist_method: a string indicating the method used for calculating the distances between pixels in image.\n","        It can be either 'Euclidean' or 'Manhattan'.\n","    save_image_size: size of images (in inches) for saving visual results.\n","    max_step: the maximum number of iterations that the IGTD algorithm will run if never converges.\n","    val_step: the number of iterations for determining algorithm convergence. If the error reduction rate is smaller than\n","        min_gain for val_step iterations, the algorithm converges.\n","    normDir: a string indicating the directory to save result files.\n","    error: a string indicating the function to evaluate the difference between feature distance ranking and pixel\n","        distance ranking. 'abs' indicates the absolute function. 'squared' indicates the square function.\n","    switch_t: the threshold on error change rate. Error change rate is\n","        (error before feature swapping - error after feature swapping) / error before feature swapping.\n","        In each iteration, if the largest error change rate resulted from all possible feature swappings\n","        is not smaller than switch_t, the feature swapping resulting in the largest error change rate will\n","        be performed. If switch_t >= 0, the IGTD algorithm monotonically reduces the error during optimization.\n","    min_gain: if the error reduction rate is not larger than min_gain for val_step iterations, the algorithm converges.\n","\n","    Return:\n","    This function does not return any variable, but saves multiple result files, which are the following\n","    1.  Results.pkl stores the original tabular data, the generated image data, and the names of samples. The generated\n","        image data is a 3D numpy array. Its size is [number of pixel rows in image, number of pixel columns in image,\n","        number of samples]. The range of values is [0, 255]. Small values in the array actually correspond to high\n","        values in the tabular data.\n","    2.  Results_Auxiliary.pkl stores the ranking matrix of pairwise feature distances before optimization,\n","        the ranking matrix of pairwise pixel distances, the coordinates of pixels when concatenating pixels\n","        row by row from image to form the pixel distance ranking matrix, error in each iteration,\n","        and time (in seconds) when completing each iteration.\n","    3.  original_feature_ranking.png shows the feature distance ranking matrix before optimization.\n","    4.  image_ranking.png shows the pixel distance ranking matrix.\n","    5.  error_and_runtime.png shows the change of error vs. time during the optimization process.\n","    6.  error_and_iteration.png shows the change of error vs. iteration during the optimization process.\n","    7.  optimized_feature_ranking.png shows the feature distance ranking matrix after optimization.\n","    8.  data folder includes two image data files for each sample. The txt file is the image data in matrix format,\n","        in which high values correspond to high values of features in tabular data. The png file shows the\n","        visualization of image data, in which black and white correspond to high and low values of features in\n","        tabular data, respectively.\n","    '''\n","\n","    if os.path.exists(normDir):\n","        shutil.rmtree(normDir)\n","    os.mkdir(normDir)\n","\n","    ranking_feature, corr = generate_feature_distance_ranking(data=norm_d, method=fea_dist_method)\n","    fig = plt.figure(figsize=(save_image_size, save_image_size))\n","    plt.imshow(np.max(ranking_feature) - ranking_feature, cmap='gray', interpolation='nearest')\n","    plt.savefig(fname=normDir + '/original_feature_ranking.png', bbox_inches='tight', pad_inches=0)\n","    plt.close(fig)\n","\n","    coordinate, ranking_image = generate_matrix_distance_ranking(num_r=scale[0], num_c=scale[1],\n","                                                                 method=image_dist_method, num=norm_d.shape[1])\n","    fig = plt.figure(figsize=(save_image_size, save_image_size))\n","    plt.imshow(np.max(ranking_image) - ranking_image, cmap='gray', interpolation='nearest')\n","    plt.savefig(fname=normDir + '/image_ranking.png', bbox_inches='tight', pad_inches=0)\n","    plt.close(fig)\n","\n","    index, err, time = IGTD(source=ranking_feature, target=ranking_image,\n","        err_measure=error, max_step=max_step, switch_t=switch_t, val_step=val_step, min_gain=min_gain, random_state=1,\n","        save_folder=normDir + '/' + error, file_name='')\n","\n","    fig = plt.figure()\n","    plt.plot(time, err)\n","    plt.savefig(fname=normDir + '/error_and_runtime.png', bbox_inches='tight', pad_inches=0)\n","    plt.close(fig)\n","    fig = plt.figure()\n","    plt.plot(range(len(err)), err)\n","    plt.savefig(fname=normDir + '/error_and_iteration.png', bbox_inches='tight', pad_inches=0)\n","    plt.close(fig)\n","    min_id = np.argmin(err)\n","    ranking_feature_random = ranking_feature[index[min_id, :], :]\n","    ranking_feature_random = ranking_feature_random[:, index[min_id, :]]\n","\n","    fig = plt.figure(figsize=(save_image_size, save_image_size))\n","    plt.imshow(np.max(ranking_feature_random) - ranking_feature_random, cmap='gray',\n","               interpolation='nearest')\n","    plt.savefig(fname=normDir + '/optimized_feature_ranking.png', bbox_inches='tight', pad_inches=0)\n","    plt.close(fig)\n","\n","    data, samples = generate_image_data(data=norm_d, index=index[min_id, :], num_row=scale[0], num_column=scale[1],\n","        coord=coordinate, image_folder=normDir + '/data', file_name='')\n","\n","    output = open(normDir + '/Results.pkl', 'wb')\n","    cp.dump(norm_d, output)\n","    cp.dump(data, output)\n","    cp.dump(samples, output)\n","    output.close()\n","\n","    output = open(normDir + '/Results_Auxiliary.pkl', 'wb')\n","    cp.dump(ranking_feature, output)\n","    cp.dump(ranking_image, output)\n","    cp.dump(coordinate, output)\n","    cp.dump(err, output)\n","    cp.dump(time, output)\n","    output.close()"],"metadata":{"id":"PcyD3Oh9JQdS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Execute Tabular to Image Code"],"metadata":{"id":"3X3GLI87LnPm"}},{"cell_type":"markdown","source":["For Training Data"],"metadata":{"id":"UMLcZddSY9Jz"}},{"cell_type":"code","source":["# This script provides examples to convert a data table into images, which can be modelled by CNN models.\n","\n","import pandas as pd\n","import os\n","#from IGTD_Functions import min_max_transform, table_to_image, select_features_by_variation\n","\n","\n","\n","num_row = 12    # Number of pixel rows in image representation\n","num_col = 12    # Number of pixel columns in image representation\n","num = num_row * num_col # Number of features to be included for analysis, which is also the total number of pixels in image representation\n","save_image_size = 2 # Size of pictures (in inches) saved during the execution of IGTD algorithm.\n","max_step = 30000    # The maximum number of iterations to run the IGTD algorithm, if it does not converge.\n","val_step = 300  # The number of iterations for determining algorithm convergence. If the error reduction rate\n","                # is smaller than a pre-set threshold for val_step itertions, the algorithm converges.\n","\n","\n","# Select features with large variations across samples\n","id = select_features_by_variation(X_train, variation_measure='var', num=num, draw_histogram=True)\n","X_train = X_train.iloc[:, id]\n","# Perform min-max transformation so that the maximum and minimum values of every feature become 1 and 0, respectively.\n","train_norm_data = min_max_transform(X_train.values)\n","train_norm_data = pd.DataFrame(train_norm_data, columns=X_train.columns, index=X_train.index)\n","\n","# Run the IGTD algorithm using (1) the Euclidean distance for calculating pairwise feature distances and pariwise pixel\n","# distances and (2) the absolute function for evaluating the difference between the feature distance ranking matrix and\n","# the pixel distance ranking matrix. Save the result in Test_1 folder.\n","fea_dist_method = 'Euclidean'\n","image_dist_method = 'Euclidean'\n","error = 'abs'\n","result_dir = '/content/drive/My Drive/Colab Notebooks/Results/Table_To_Image_Conversion/Test_1'\n","os.makedirs(name=result_dir, exist_ok=True)\n","table_to_image(train_norm_data, [num_row, num_col], fea_dist_method, image_dist_method, save_image_size,\n","               max_step, val_step, result_dir, error)\n","\n","'''\n","# Run the IGTD algorithm using (1) the Pearson correlation coefficient for calculating pairwise feature distances,\n","# (2) the Manhattan distance for calculating pariwise pixel distances, and (3) the square function for evaluating\n","# the difference between the feature distance ranking matrix and the pixel distance ranking matrix.\n","# Save the result in Test_2 folder.\n","fea_dist_method = 'Pearson'\n","image_dist_method = 'Manhattan'\n","error = 'squared'\n","norm_data = norm_data.iloc[:, :800]\n","result_dir = '/content/drive/My Drive/Colab Notebooks/Results/Table_To_Image_Conversion/Test_2'\n","os.makedirs(name=result_dir, exist_ok=True)\n","table_to_image(norm_data, [num_row, num_col], fea_dist_method, image_dist_method, save_image_size,\n","               max_step, val_step, result_dir, error)\n","               '''"],"metadata":{"id":"KSl02GvFY5FJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For Test Data"],"metadata":{"id":"YsX9x-1iZAbs"}},{"cell_type":"code","source":["# This script provides examples to convert a data table into images, which can be modelled by CNN models.\n","\n","import pandas as pd\n","import os\n","#from IGTD_Functions import min_max_transform, table_to_image, select_features_by_variation\n","\n","\n","\n","\n","# Select features with large variations across samples\n","id = select_features_by_variation(X_test, variation_measure='var', num=num)\n","X_test = X_test.iloc[:, id]\n","# Perform min-max transformation so that the maximum and minimum values of every feature become 1 and 0, respectively.\n","test_norm_data = min_max_transform(X_test.values)\n","test_norm_data = pd.DataFrame(test_norm_data, columns=X_test.columns, index=X_test.index)\n","\n","# Run the IGTD algorithm using (1) the Euclidean distance for calculating pairwise feature distances and pariwise pixel\n","# distances and (2) the absolute function for evaluating the difference between the feature distance ranking matrix and\n","# the pixel distance ranking matrix. Save the result in Test_1 folder.\n","fea_dist_method = 'Euclidean'\n","image_dist_method = 'Euclidean'\n","error = 'abs'\n","result_dir = '/content/drive/My Drive/Colab Notebooks/Results/Table_To_Image_Conversion/Test_2'\n","os.makedirs(name=result_dir, exist_ok=True)\n","table_to_image(test_norm_data, [num_row, num_col], fea_dist_method, image_dist_method, save_image_size,\n","               max_step, val_step, result_dir, error)\n","\n","'''\n","# Run the IGTD algorithm using (1) the Pearson correlation coefficient for calculating pairwise feature distances,\n","# (2) the Manhattan distance for calculating pariwise pixel distances, and (3) the square function for evaluating\n","# the difference between the feature distance ranking matrix and the pixel distance ranking matrix.\n","# Save the result in Test_2 folder.\n","fea_dist_method = 'Pearson'\n","image_dist_method = 'Manhattan'\n","error = 'squared'\n","norm_data = norm_data.iloc[:, :800]\n","result_dir = '/content/drive/My Drive/Colab Notebooks/Results/Table_To_Image_Conversion/Test_2'\n","os.makedirs(name=result_dir, exist_ok=True)\n","table_to_image(norm_data, [num_row, num_col], fea_dist_method, image_dist_method, save_image_size,\n","               max_step, val_step, result_dir, error)\n","               '''"],"metadata":{"id":"CLb1qGOyLtuy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# CNN Code"],"metadata":{"id":"Zs0xMIBxL1Ym"}},{"cell_type":"code","source":["from keras import backend\n","from keras import optimizers\n","from keras.models import Model, load_model\n","from keras.layers import Input, Dense, Dropout, concatenate, Conv2D, BatchNormalization, ReLU, MaxPooling2D, \\\n","    Flatten, AlphaDropout\n","from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping\n","from scipy import stats\n","from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, roc_auc_score, accuracy_score, \\\n","    matthews_corrcoef\n","\n","import configparser\n","import numpy as np\n","import keras\n","import os\n","import pandas as pd\n","import shutil\n","\n","\n","\n","def ID_mapping(l1, l2):\n","    pos = {}\n","    for i in range(len(l1)):\n","        pos[l1[i]] = i\n","    idd = np.array([pos[i] for i in l2])\n","    return idd\n","\n","\n","\n","def load_example_data():\n","    res = pd.read_csv('../Data/Example_Drug_Response_Data.txt', sep='\\t', engine='c',\n","                      na_values=['na', '-', ''], header=0, index_col=None)\n","\n","    files = os.listdir('../Data/Example_Drug_Descriptor_Image_Data/')\n","    image = np.empty((len(files), 50, 50, 1))\n","    sample = []\n","    id = []\n","    for i in range(len(files)):\n","        if files[i].split('.')[1] == 'txt' and files[i].split('_')[0] == 'Drug':\n","            id.append(i)\n","            data = pd.read_csv('../Data/Example_Drug_Descriptor_Image_Data/' + files[i], sep='\\t', engine='c',\n","                               na_values=['na', '-', ''], header=None, index_col=None)\n","            image[i, :, :, 0] = data.values\n","            sample.append(files[i].split('.txt')[0])\n","    image = image[id, :, :, :]\n","    drug = {}\n","    drug['data'] = image\n","    drug['sample'] = sample\n","\n","    files = os.listdir('../Data/Example_Gene_Expression_Image_Data/')\n","    image = np.empty((len(files), 50, 50, 1))\n","    sample = []\n","    id = []\n","    for i in range(len(files)):\n","        if files[i].split('.')[1] == 'txt' and files[i].split('_')[0] == 'CCL':\n","            id.append(i)\n","            data = pd.read_csv('../Data/Example_Gene_Expression_Image_Data/' + files[i], sep='\\t', engine='c',\n","                               na_values=['na', '-', ''], header=None, index_col=None)\n","            image[i, :, :, 0] = data.values\n","            sample.append(files[i].split('.txt')[0])\n","    image = image[id, :, :, :]\n","    ccl = {}\n","    ccl['data'] = image\n","    ccl['sample'] = sample\n","\n","    return res, ccl, drug\n","\n","\n","\n","def get_data_for_cross_validation(res, ccl, drug, sampleID):\n","\n","    trainData = []\n","    valData = []\n","    testData = []\n","\n","    train_idd = ID_mapping(drug['sample'], res.iloc[sampleID['trainID'], :].Drug)\n","    trainData.append(drug['data'][train_idd, :, :, :])\n","    val_idd = ID_mapping(drug['sample'], res.iloc[sampleID['valID'], :].Drug)\n","    valData.append(drug['data'][val_idd, :, :, :])\n","    test_idd = ID_mapping(drug['sample'], res.iloc[sampleID['testID'], :].Drug)\n","    testData.append(drug['data'][test_idd, :, :, :])\n","\n","    train_idd = ID_mapping(ccl['sample'], res.iloc[sampleID['trainID'], :].CCL)\n","    trainData.append(ccl['data'][train_idd, :, :, :])\n","    val_idd = ID_mapping(ccl['sample'], res.iloc[sampleID['valID'], :].CCL)\n","    valData.append(ccl['data'][val_idd, :, :, :])\n","    test_idd = ID_mapping(ccl['sample'], res.iloc[sampleID['testID'], :].CCL)\n","    testData.append(ccl['data'][test_idd, :, :, :])\n","\n","    trainLabel = res.iloc[sampleID['trainID'], :].AUC.values\n","    valLabel = res.iloc[sampleID['valID'], :].AUC.values\n","    testLabel = res.iloc[sampleID['testID'], :].AUC.values\n","\n","    train = {}\n","    train['data'] = trainData\n","    train['label'] = trainLabel\n","    train['sample'] = res.iloc[sampleID['trainID'], :].CCL + '|' + res.iloc[sampleID['trainID'], :].Drug\n","    val = {}\n","    val['data'] = valData\n","    val['label'] = valLabel\n","    val['sample'] = res.iloc[sampleID['valID'], :].CCL + '|' + res.iloc[sampleID['valID'], :].Drug\n","    test = {}\n","    test['data'] = testData\n","    test['label'] = testLabel\n","    test['sample'] = res.iloc[sampleID['testID'], :].CCL + '|' + res.iloc[sampleID['testID'], :].Drug\n","\n","    return train, val, test\n","\n","\n","\n","def get_model_parameter(model_file):\n","    config = configparser.ConfigParser()\n","    config.read(model_file)\n","    section = config.sections()\n","    params = {}\n","    for sec in section:\n","        for k, v in config.items(sec):\n","            if k not in params:\n","                params[k] = eval(v)\n","    return params\n","\n","\n","\n","def get_DNN_optimizer(opt_name):\n","    if opt_name == 'SGD':\n","        optimizer = optimizers.GSD()\n","    elif opt_name == 'SGD_momentum':\n","        optimizer = optimizers.GSD(momentum=0.9)\n","    elif opt_name == 'SGD_momentum_nesterov':\n","        optimizer = optimizers.GSD(momentum=0.9, nesterov=True)\n","    elif opt_name == 'RMSprop':\n","        optimizer = optimizers.RMSprop()\n","    elif opt_name == 'Adagrad':\n","        optimizer = optimizers.Adagrad()\n","    elif opt_name == 'Adadelta':\n","        optimizer = optimizers.Adadelta()\n","    elif opt_name == 'Adam':\n","        optimizer = optimizers.Adam()\n","    elif opt_name == 'Adam_amsgrad':\n","        optimizer = optimizers.Adam(amsgrad=True)\n","    else:\n","        optimizer = optimizers.Adam()\n","\n","    return optimizer\n","\n","\n","\n","def calculate_batch_size(num_sample, paraDNN):\n","    # max_half_num_batch: the number of batches will not be larger than 2 * max_half_num_batch\n","    max_half_num_batch = paraDNN['max_half_num_batch']\n","    if num_sample < max_half_num_batch * 4:\n","        batch_size = 2\n","    elif num_sample < max_half_num_batch * 8:\n","        batch_size = 4\n","    elif num_sample < max_half_num_batch * 16:\n","        batch_size = 8\n","    elif num_sample < max_half_num_batch * 32:\n","        batch_size = 16\n","    elif num_sample < max_half_num_batch * 64:\n","        batch_size = 32\n","    elif num_sample < max_half_num_batch * 128:\n","        batch_size = 64\n","    elif num_sample < max_half_num_batch * 256:\n","        batch_size = 128\n","    else:\n","        batch_size = 256\n","\n","    return batch_size\n","\n","\n","\n","class CNN2D_Regressor():\n","    # This is the class for 2-dimensional convolutional neural network regressors (CNN2D_Regressor).\n","    # The model can accept more than one sizes of filter, such [3, 5].\n","    def __init__(self, params, input_data_dim, dropout):\n","        # params: dict, CNN2D model parameters\n","        # input_data_dim: a list. Each element of the list includes two positive integers, the dimension of input images\n","        # dropout: dropout rate, all layers use the same dropout rate\n","\n","        self.params = params\n","        self.dropout = dropout\n","        self.input_data_dim = input_data_dim\n","\n","        num_kernel_size = len(self.params['kernel_size'])\n","        num_conv_layer = []\n","        for i in range(num_kernel_size):\n","            num_conv_layer.append(len(self.params['num_kernel'][i]))\n","        num_dense_layer = len(self.params['network_layers'])\n","\n","        input = []\n","        input2List = []\n","        num_input = len(self.input_data_dim)\n","        for input_id in range(num_input):\n","            in_id = Input(shape=(self.input_data_dim[input_id][0], self.input_data_dim[input_id][1], 1),\n","                          name='Input_' + str(input_id))\n","            input.append(in_id)\n","            for j in range(num_kernel_size):\n","                min_row_size = self.params['pool_size'][j][0] * 2 + self.params['kernel_size'][j][0] - 1\n","                min_col_size = self.params['pool_size'][j][1] * 2 + self.params['kernel_size'][j][1] - 1\n","                for i in range(num_conv_layer[j]):\n","                    if i == 0:\n","                        d = Conv2D(filters=self.params['num_kernel'][j][i], kernel_size=self.params['kernel_size'][j],\n","                                   strides=self.params['strides'][j], padding='valid', data_format='channels_last',\n","                                   name='Conv2D_' + str(i) + '_Kernel_' + str(j) + '_Input_' + str(input_id))(in_id)\n","                    else:\n","                        d = Conv2D(filters=self.params['num_kernel'][j][i], kernel_size=self.params['kernel_size'][j],\n","                                   strides=self.params['strides'][j], padding='valid', data_format='channels_last',\n","                                   name='Conv2D_' + str(i) + '_Kernel_' + str(j) + '_Input_' + str(input_id))(d)\n","                    d = BatchNormalization(axis=-1, name='BatchNorm_' + str(i) + '_Kernel_' + str(j) + '_Input_'\n","                                                         + str(input_id))(inputs=d)\n","                    if self.params['subnetwork_activation'] == 'relu':\n","                        d = ReLU(name='ReLU_' + str(i) + '_Kernel_' + str(j) + '_Input_' + str(input_id))(d)\n","                    else:\n","                        raise TypeError(\"Activation is not ReLU in subnetwork.\")\n","                    d = MaxPooling2D(pool_size=self.params['pool_size'][j], name='MaxPooling_' + str(i) + '_Kernel_'\n","                        + str(j) + '_Input_' + str(input_id))(d)\n","                    dim = np.array(d.shape.as_list())\n","                    flag_0 = dim[1] < min_row_size\n","                    flag_1 = dim[2] < min_col_size\n","                    if flag_0 or flag_1:\n","                        break\n","                d = Flatten()(d)\n","                input2List.append(d)\n","\n","        if num_input > 1:\n","            d = concatenate(input2List, name='concatenation')\n","        for i in range(num_dense_layer):\n","            if self.params['activation'] == 'selu':\n","                d = Dense(self.params['network_layers'][i], activation=self.params['activation'], name='Dense_' + str(i),\n","                          kernel_initializer='lecun_normal')(d)\n","            else:\n","                d = Dense(self.params['network_layers'][i], activation=self.params['activation'], name='Dense_' + str(i))(d)\n","            if i != num_dense_layer - 1:\n","                if self.params['activation'] == 'selu':\n","                    d = AlphaDropout(self.dropout, name='Dropout_Dense_' + str(i))(d)\n","                else:\n","                    d = Dropout(self.dropout, name='Dropout_Dense_' + str(i))(d)\n","\n","        output = Dense(1, name='output')(d)\n","        if num_input > 1:\n","            model = Model(inputs=input, outputs=output)\n","        else:\n","            model = Model(inputs=input[0], outputs=output)\n","        model.compile(optimizer=get_DNN_optimizer(self.params['optimizer']), loss=self.params['loss'])\n","        print(model.summary())\n","        self.model = model\n","\n","\n","\n","class CNN2D_Classifier():\n","    # This is the class for 2-dimensional convolutional neural network regressors (CNN2D_Regressor).\n","    # The model can accept more than one sizes of filter, such [3, 5].\n","    def __init__(self, params, input_data_dim, num_class, dropout):\n","        # params: dict, CNN2D model parameters\n","        # input_data_dim: a list. Each element of the list includes two positive integers, the dimension of input images\n","        # dropout: dropout rate, all layers use the same dropout rate\n","\n","        self.params = params\n","        self.dropout = dropout\n","        self.num_class = num_class\n","        self.input_data_dim = input_data_dim\n","\n","        num_kernel_size = len(self.params['kernel_size'])\n","        num_conv_layer = []\n","        for i in range(num_kernel_size):\n","            num_conv_layer.append(len(self.params['num_kernel'][i]))\n","        num_dense_layer = len(self.params['network_layers'])\n","\n","        input = []\n","        input2List = []\n","        num_input = len(self.input_data_dim)\n","        for input_id in range(num_input):\n","            in_id = Input(shape=(self.input_data_dim[input_id][0], self.input_data_dim[input_id][1], 1),\n","                          name='Input_' + str(input_id))\n","            input.append(in_id)\n","            for j in range(num_kernel_size):\n","                min_row_size = self.params['pool_size'][j][0] * 2 + self.params['kernel_size'][j][0] - 1\n","                min_col_size = self.params['pool_size'][j][1] * 2 + self.params['kernel_size'][j][1] - 1\n","                for i in range(num_conv_layer[j]):\n","                    if i == 0:\n","                        d = Conv2D(filters=self.params['num_kernel'][j][i], kernel_size=self.params['kernel_size'][j],\n","                                   strides=self.params['strides'][j], padding='valid', data_format='channels_last',\n","                                   name='Conv2D_' + str(i) + '_Kernel_' + str(j) + '_Input_' + str(input_id))(in_id)\n","                    else:\n","                        d = Conv2D(filters=self.params['num_kernel'][j][i], kernel_size=self.params['kernel_size'][j],\n","                                   strides=self.params['strides'][j], padding='valid', data_format='channels_last',\n","                                   name='Conv2D_' + str(i) + '_Kernel_' + str(j) + '_Input_' + str(input_id))(d)\n","                    d = BatchNormalization(axis=-1, name='BatchNorm_' + str(i) + '_Kernel_' + str(j) + '_Input_'\n","                                                         + str(input_id))(inputs=d)\n","                    if self.params['subnetwork_activation'] == 'relu':\n","                        d = ReLU(name='ReLU_' + str(i) + '_Kernel_' + str(j) + '_Input_' + str(input_id))(d)\n","                    else:\n","                        raise TypeError(\"Activation is not ReLU in subnetwork.\")\n","                    d = MaxPooling2D(pool_size=self.params['pool_size'][j], name='MaxPooling_' + str(i) + '_Kernel_'\n","                        + str(j) + '_Input_' + str(input_id), padding='same')(d)\n","                    dim = np.array(d.shape.as_list())\n","                    flag_0 = dim[1] < min_row_size\n","                    flag_1 = dim[2] < min_col_size\n","                    if flag_0 or flag_1:\n","                        break\n","                d = Flatten()(d)\n","                input2List.append(d)\n","\n","        if num_input > 1:\n","            d = concatenate(input2List, name='concatenation')\n","        for i in range(num_dense_layer):\n","            if self.params['activation'] == 'selu':\n","                d = Dense(self.params['network_layers'][i], activation=self.params['activation'], name='Dense_' + str(i),\n","                          kernel_initializer='lecun_normal')(d)\n","            else:\n","                d = Dense(self.params['network_layers'][i], activation=self.params['activation'], name='Dense_' + str(i))(d)\n","            if i != num_dense_layer - 1:\n","                if self.params['activation'] == 'selu':\n","                    d = AlphaDropout(self.dropout, name='Dropout_Dense_' + str(i))(d)\n","                else:\n","                    d = Dropout(self.dropout, name='Dropout_Dense_' + str(i))(d)\n","\n","        output = Dense(self.num_class, activation='softmax', name='output')(d)\n","        if num_input > 1:\n","            model = Model(inputs=input, outputs=output)\n","        else:\n","            model = Model(inputs=input[0], outputs=output)\n","        model.compile(optimizer=get_DNN_optimizer(self.params['optimizer']), loss=self.params['loss'])\n","        print(model.summary())\n","        self.model = model\n","\n","\n","\n","def CNN2D_Regression_Analysis(train, resultFolder, para, val=None, test=None):\n","    '''\n","    This function does CNN2D regression analysis without HPO.\n","\n","    Input:\n","    train: a dictionary of three elements. data is an array of (sample, height, width).\n","        label is a series of the prediction target. sample is an array of sample names.\n","    val: a dictionary for validation data.\n","    resultFolder: directory to save models, features, and results\n","    para: parameters used for model training\n","    test: a dictionary for testing data. Default is None.\n","\n","    Return:\n","    predResult: a dictionary including three series, which are prediction results on the training, validation,\n","        and testing sets.\n","    perM: an array of training and validation losses with different dropout rates and epochs.\n","    perf: a 3 by 7 data frame including the prediction performance on training, validation, and testing sets.\n","    winningModel: a string giving the epoch number and dropout rate of the best model with the smallest validation loss.\n","    '''\n","\n","    if os.path.exists(resultFolder):\n","        shutil.rmtree(resultFolder)\n","    os.mkdir(resultFolder)\n","\n","    trainData = train['data']\n","    trainLabel = train['label']\n","    trainSample = train['sample']\n","\n","    if isinstance(trainData, list):\n","        batch_size = calculate_batch_size(trainData[0].shape[0], para)\n","    else:\n","        batch_size = calculate_batch_size(trainData.shape[0], para)\n","\n","    # batch_size = 5000\n","    # print(batch_size)\n","\n","    if val is not None:\n","        valData = val['data']\n","        valLabel = val['label']\n","        valSample = val['sample']\n","    else:\n","        valData = None\n","        valLabel = None\n","        valSample = None\n","\n","    if test is not None:\n","        testData = test['data']\n","        testSample = test['sample']\n","        if test['label'] is not None:\n","            testLabel = test['label']\n","        else:\n","            testLabel = None\n","    else:\n","        testData = None\n","        testLabel = None\n","        testSample = None\n","\n","    if isinstance(trainData, list):\n","        input_data_dim = []\n","        for i in range(len(trainData)):\n","            input_data_dim.append([trainData[i].shape[1], trainData[i].shape[2]])\n","    else:\n","        input_data_dim = [[trainData.shape[1], trainData.shape[2]]]\n","\n","    perM = {}\n","    for i in ['train', 'val']:\n","        perM[i] = np.empty((len(para['drop']), para['epochs']))\n","        perM[i].fill(np.inf)\n","        perM[i] = pd.DataFrame(perM[i], index=['dropout_' + str(j) for j in para['drop']],\n","            columns=['epoch_' + str(j) for j in range(para['epochs'])])\n","\n","    for dpID in range(len(para['drop'])):\n","        label = 'dropout_' + str(para['drop'][dpID])\n","        print(label)\n","\n","        if val is not None:\n","            monitor = 'val_loss'\n","        else:\n","            monitor = 'loss'\n","        train_logger = CSVLogger(resultFolder + '/log_dropout_' + str(para['drop'][dpID]) + '.csv')\n","        model_saver = ModelCheckpoint(resultFolder + '/model_dropout_' + str(para['drop'][dpID]) + '.h5',\n","                                      monitor=monitor, save_best_only=True, save_weights_only=False)\n","        reduce_lr = ReduceLROnPlateau(monitor=monitor, factor=para['rlr_factor'], patience=para['rlr_patience'],\n","                                      verbose=1, mode='auto', min_delta=para['rlr_min_delta'],\n","                                      cooldown=para['rlr_cooldown'], min_lr=para['rlr_min_lr'])\n","        early_stop = EarlyStopping(monitor=monitor, patience=para['es_patience'], min_delta=para['es_min_delta'],\n","                                   verbose=1)\n","        callbacks = [model_saver, train_logger, reduce_lr, early_stop]\n","\n","        temp = CNN2D_Regressor(para, input_data_dim, para['drop'][dpID])\n","\n","        if val is not None:\n","            history = temp.model.fit(x=trainData, y=trainLabel, batch_size=batch_size, epochs=para['epochs'],\n","                verbose=para['verbose'], callbacks=callbacks, validation_data=(valData, valLabel), shuffle=True)\n","        else:\n","            history = temp.model.fit(x=trainData, y=trainLabel, batch_size=batch_size, epochs=para['epochs'],\n","                verbose=para['verbose'], callbacks=callbacks, validation_data=None, shuffle=True)\n","        numEpoch = len(history.history['loss'])\n","        i = np.where(perM['train'].index == label)[0]\n","        perM['train'].iloc[i, :numEpoch] = history.history['loss']\n","        if val is not None:\n","            numEpoch = len(history.history['val_loss'])\n","            i = np.where(perM['val'].index == label)[0]\n","            perM['val'].iloc[i, :numEpoch] = history.history['val_loss']\n","\n","        backend.clear_session()\n","\n","    if val is not None:\n","        dpID, epID = np.unravel_index(np.argmin(perM['val'].values, axis=None), perM['val'].shape)\n","    else:\n","        dpID, epID = np.unravel_index(np.argmin(perM['train'].values, axis=None), perM['train'].shape)\n","    model = load_model(resultFolder + '/model_dropout_' + str(para['drop'][dpID]) + '.h5')\n","\n","    for i in range(len(para['drop'])):\n","        if i == dpID:\n","            continue\n","        os.remove(resultFolder + '/model_dropout_' + str(para['drop'][i]) + '.h5')\n","        os.remove(resultFolder + '/log_dropout_' + str(para['drop'][i]) + '.csv')\n","\n","    predResult = {}\n","    if test is not None:\n","        predResult['test'] = pd.DataFrame(model.predict(testData), index=testSample, columns=['prediction'])\n","    predResult['train'] = pd.DataFrame(model.predict(trainData), index=trainSample, columns=['prediction'])\n","    if val is not None:\n","        predResult['val'] = pd.DataFrame(model.predict(valData), index=valSample, columns=['prediction'])\n","\n","    backend.clear_session()\n","\n","    perf = np.empty((3, 7))\n","    perf.fill(np.nan)\n","    perf = pd.DataFrame(perf, columns=['R2', 'MSE', 'MAE', 'pCor', 'pCorPvalue', 'sCor', 'sCorPvalue'],\n","                        index=['train', 'val', 'test'])\n","    for k in ['train', 'val', 'test']:\n","        if (eval(k + 'Data') is None) or (eval(k + 'Label') is None):\n","            continue\n","        perf.loc[k, 'R2'] = r2_score(eval(k + 'Label'), predResult[k].values[:, 0])\n","        perf.loc[k, 'MSE'] = mean_squared_error(eval(k + 'Label'), predResult[k].values[:, 0])\n","        perf.loc[k, 'MAE'] = mean_absolute_error(eval(k + 'Label'), predResult[k].values[:, 0])\n","        rho, pval = stats.pearsonr(eval(k + 'Label'), predResult[k].values[:, 0])\n","        perf.loc[k, 'pCor'] = rho\n","        perf.loc[k, 'pCorPvalue'] = pval\n","        rho, pval = stats.spearmanr(eval(k + 'Label'), predResult[k].values[:, 0])\n","        perf.loc[k, 'sCor'] = rho\n","        perf.loc[k, 'sCorPvalue'] = pval\n","\n","    return predResult, perM, perf, 'dropout_' + str(para['drop'][dpID]) + '_epoch_' + str(epID + 1), batch_size\n","\n","\n","\n","def CNN2D_Classification_Analysis(train, num_class, resultFolder, para, class_weight=None, val=None, test=None):\n","    '''\n","    This function does CNN2D regression analysis without HPO.\n","\n","    Input:\n","    train: a dictionary of three elements. data is an array of (sample, height, width).\n","        label is a series of the prediction target. sample is an array of sample names.\n","    val: a dictionary for validation data.\n","    resultFolder: directory to save models, features, and results\n","    para: parameters used for model training\n","    test: a dictionary for testing data. Default is None.\n","\n","    Return:\n","    predResult: a dictionary including three series, which are prediction results on the training, validation,\n","        and testing sets.\n","    perM: an array of training and validation losses with different dropout rates and epochs.\n","    perf: a 3 by 7 data frame including the prediction performance on training, validation, and testing sets.\n","    winningModel: a string giving the epoch number and dropout rate of the best model with the smallest validation loss.\n","    '''\n","\n","    if os.path.exists(resultFolder):\n","        shutil.rmtree(resultFolder)\n","    os.mkdir(resultFolder)\n","\n","    trainData = train['data']\n","    trainLabel = train['label']\n","    trainSample = train['sample']\n","\n","    if isinstance(trainData, list):\n","        batch_size = calculate_batch_size(trainData[0].shape[0], para)\n","    else:\n","        batch_size = calculate_batch_size(trainData.shape[0], para)\n","    # batch_size = 5000\n","    # print(batch_size)\n","\n","    if val is not None:\n","        valData = val['data']\n","        valLabel = val['label']\n","        valSample = val['sample']\n","    else:\n","        valData = None\n","        valLabel = None\n","        valSample = None\n","\n","    if test is not None:\n","        testData = test['data']\n","        testSample = test['sample']\n","        if test['label'] is not None:\n","            testLabel = test['label']\n","        else:\n","            testLabel = None\n","    else:\n","        testData = None\n","        testLabel = None\n","        testSample = None\n","\n","\n","    if isinstance(trainData, list):\n","        input_data_dim = []\n","        for i in range(len(trainData)):\n","            input_data_dim.append([trainData[i].shape[1], trainData[i].shape[2]])\n","    else:\n","        input_data_dim = [[trainData.shape[1], trainData.shape[2]]]\n","\n","    perM = {}\n","    for i in ['train', 'val']:\n","        perM[i] = np.empty((len(para['drop']), para['epochs']))\n","        perM[i].fill(np.inf)\n","        perM[i] = pd.DataFrame(perM[i], index=['dropout_' + str(j) for j in para['drop']],\n","            columns=['epoch_' + str(j) for j in range(para['epochs'])])\n","\n","    if class_weight == 'balanced':\n","        weight = len(trainLabel) / (num_class * np.bincount(trainLabel))\n","        class_weight = {}\n","        for i in range(num_class):\n","            class_weight[i] = weight[i]\n","\n","    for dpID in range(len(para['drop'])):\n","        label = 'dropout_' + str(para['drop'][dpID])\n","        print(label)\n","\n","        if val is not None:\n","            monitor = 'val_loss'\n","        else:\n","            monitor = 'loss'\n","        train_logger = CSVLogger(resultFolder + '/log_dropout_' + str(para['drop'][dpID]) + '.csv')\n","        model_saver = ModelCheckpoint(resultFolder + '/model_dropout_' + str(para['drop'][dpID]) + '.h5',\n","                                      monitor=monitor, save_best_only=True, save_weights_only=False)\n","        reduce_lr = ReduceLROnPlateau(monitor=monitor, factor=para['rlr_factor'], patience=para['rlr_patience'],\n","                                      verbose=1, mode='auto', min_delta=para['rlr_min_delta'],\n","                                      cooldown=para['rlr_cooldown'], min_lr=para['rlr_min_lr'])\n","        early_stop = EarlyStopping(monitor=monitor, patience=para['es_patience'], min_delta=para['es_min_delta'],\n","                                   verbose=1)\n","        callbacks = [model_saver, train_logger, reduce_lr, early_stop]\n","\n","        temp = CNN2D_Classifier(para, input_data_dim, num_class, para['drop'][dpID])\n","\n","        if val is not None:\n","            history = temp.model.fit(x=trainData, y=trainLabel, batch_size=batch_size, epochs=para['epochs'],\n","                verbose=para['verbose'], callbacks=callbacks, validation_data=(valData, valLabel),\n","                class_weight=class_weight, shuffle=True)\n","        else:\n","            history = temp.model.fit(x=trainData, y=trainLabel, batch_size=batch_size, epochs=para['epochs'],\n","                verbose=para['verbose'], callbacks=callbacks, validation_data=None, class_weight=class_weight,\n","                shuffle=True)\n","        numEpoch = len(history.history['loss'])\n","        i = np.where(perM['train'].index == label)[0]\n","        perM['train'].iloc[i, :numEpoch] = history.history['loss']\n","        if val is not None:\n","            numEpoch = len(history.history['val_loss'])\n","            i = np.where(perM['val'].index == label)[0]\n","            perM['val'].iloc[i, :numEpoch] = history.history['val_loss']\n","\n","        backend.clear_session()\n","\n","    if val is not None:\n","        dpID, epID = np.unravel_index(np.argmin(perM['val'].values, axis=None), perM['val'].shape)\n","    else:\n","        dpID, epID = np.unravel_index(np.argmin(perM['train'].values, axis=None), perM['train'].shape)\n","    model = load_model(resultFolder + '/model_dropout_' + str(para['drop'][dpID]) + '.h5')\n","\n","    for i in range(len(para['drop'])):\n","        if i == dpID:\n","            continue\n","        os.remove(resultFolder + '/model_dropout_' + str(para['drop'][i]) + '.h5')\n","        os.remove(resultFolder + '/log_dropout_' + str(para['drop'][i]) + '.csv')\n","\n","    predResult = {}\n","    if test is not None:\n","        predResult['test'] = {}\n","        predResult['test']['proba'] = pd.DataFrame(model.predict(testData), index=testSample,\n","                                                   columns=['proba_' + str(i) for i in range(num_class)])\n","        predResult['test']['label'] = pd.DataFrame(np.argmax(a=predResult['test']['proba'].values, axis=1),\n","                                                   index=predResult['test']['proba'].index, columns=['prediction'])\n","    predResult['train'] = {}\n","    predResult['train']['proba'] = pd.DataFrame(model.predict(trainData), index=trainSample,\n","                                               columns=['proba_' + str(i) for i in range(num_class)])\n","    predResult['train']['label'] = pd.DataFrame(np.argmax(a=predResult['train']['proba'].values, axis=1),\n","                                               index=predResult['train']['proba'].index, columns=['prediction'])\n","    if val is not None:\n","        predResult['val'] = {}\n","        predResult['val']['proba'] = pd.DataFrame(model.predict(valData), index=valSample,\n","                                                  columns=['proba_' + str(i) for i in range(num_class)])\n","        predResult['val']['label'] = pd.DataFrame(np.argmax(a=predResult['val']['proba'].values, axis=1),\n","                                                  index=predResult['val']['proba'].index, columns=['prediction'])\n","\n","    backend.clear_session()\n","\n","    perf = np.empty((3, 3))\n","    perf.fill(np.nan)\n","    perf = pd.DataFrame(perf, columns=['ACC', 'AUROC', 'MCC'], index=['train', 'val', 'test'])\n","    for k in ['train', 'val', 'test']:\n","        if (eval(k + 'Data') is None) or (eval(k + 'Label') is None):\n","            continue\n","        perf.loc[k, 'ACC'] = accuracy_score(eval(k + 'Label'), predResult[k]['label'].values[:, 0])\n","        if num_class == 2:\n","            perf.loc[k, 'AUROC'] = roc_auc_score(eval(k + 'Label'), predResult[k]['proba'].values[:, 1])\n","        else:\n","            perf.loc[k, 'AUROC'] = roc_auc_score(keras.utils.to_categorical(eval(k + 'Label')), predResult[k]['proba'].values,\n","                                                 labels=range(num_class), multi_class='ovr')\n","        perf.loc[k, 'MCC'] = matthews_corrcoef(eval(k + 'Label'), predResult[k]['label'].values[:, 0])\n","\n","    return predResult, perM, perf, 'dropout_' + str(para['drop'][dpID]) + '_epoch_' + str(epID + 1), batch_size"],"metadata":{"id":"wP7uNwlFFfpA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import _pickle as cp\n","import shutil\n","import os\n","\n","#from Prediction_Modeling_Functions import get_model_parameter, load_example_data, get_data_for_cross_validation, \\\n","    #CNN2D_Regression_Analysis, CNN2D_Classification_Analysis\n","\n","\n","\n","# Load the example drug response data, cell line gene expression image data, and drug descriptor image data\n","res, ccl, drug = load_example_data()\n","\n","# Generate sample IDs for 10-fold cross-validation. 8 data folds for training, 1 data fold for validation,\n","# and 1 data fold for testing\n","num_fold = 10\n","num_sample = res.shape[0]\n","rand_sample_ID = np.random.permutation(num_sample)\n","fold_size = int(num_sample / num_fold)\n","sampleID = {}\n","sampleID['trainID'] = rand_sample_ID[range(fold_size * (num_fold - 2))]\n","sampleID['valID'] = rand_sample_ID[range(fold_size * (num_fold - 2), fold_size * (num_fold - 1))]\n","sampleID['testID'] = rand_sample_ID[range(fold_size * (num_fold - 1), num_sample)]\n","\n","\n","\n","# Run regression prediction\n","# Create the directory for saving results\n","result_dir = '../Results/Prediction_On_Images/Regression_Prediction'\n","if os.path.exists(result_dir):\n","    shutil.rmtree(result_dir)\n","os.makedirs(result_dir)\n","\n","# Load network parameters\n","para = get_model_parameter('../Data/Example_Model_Parameters/FCNN_Regressor.txt')\n","subnetwork_para = get_model_parameter('../Data/Example_Model_Parameters/CNN2D_SubNetwork.txt')\n","para.update(subnetwork_para)\n","\n","# Generate data for cross-validation analysis\n","train, val, test = get_data_for_cross_validation(res, ccl, drug, sampleID)\n","\n","predResult, perM, perf, winningModel, batch_size = CNN2D_Regression_Analysis(train=train, resultFolder=result_dir,\n","                                                                             para=para, val=val, test=test)\n","\n","result = {}\n","result['predResult'] = predResult       # Prediction values for the training, validation, and testing sets\n","result['perM'] = perM                   # Loss values of training and validation during model training\n","result['perf'] = perf                   # Prediction performance metrics\n","result['winningModel'] = winningModel   # Model with the minimum validation loss\n","result['batch_size'] = batch_size       # Batch size used in model training\n","\n","# Save prediction performance and all data and results\n","perf.to_csv(result_dir + '/Prediction_Performance.txt', header=True, index=True, sep='\\t', line_terminator='\\r\\n')\n","output = open(result_dir + '/Result.pkl', 'wb')\n","cp.dump(res, output)\n","cp.dump(ccl, output)\n","cp.dump(drug, output)\n","cp.dump(result, output)\n","cp.dump(sampleID, output)\n","cp.dump(para, output)\n","output.close()\n","\n","\n","\n","# Run classification prediction\n","# Create the directory for saving results\n","result_dir = '../Results/Prediction_On_Images/Classification_Prediction'\n","if os.path.exists(result_dir):\n","    shutil.rmtree(result_dir)\n","os.makedirs(result_dir)\n","\n","# Convert AUC values into response (AUC < 0.5) and non-response (AUC >= 0.5)\n","id_pos = np.where(res.AUC < 0.5)[0]\n","id_neg = np.setdiff1d(range(res.shape[0]), id_pos)\n","res.iloc[id_pos, 2] = 1\n","res.iloc[id_neg, 2] = 0\n","res.AUC = res.AUC.astype('int64')\n","\n","# Load network parameters\n","para = get_model_parameter('../Data/Example_Model_Parameters/FCNN_Classifier.txt')\n","subnetwork_para = get_model_parameter('../Data/Example_Model_Parameters/CNN2D_SubNetwork.txt')\n","para.update(subnetwork_para)\n","\n","# Generate data for cross-validation analysis\n","train, val, test = get_data_for_cross_validation(res, ccl, drug, sampleID)\n","\n","predResult, perM, perf, winningModel, batch_size = CNN2D_Classification_Analysis(train=train, num_class=2,\n","    resultFolder=result_dir, class_weight='balanced', para=para, val=val, test=test)\n","\n","result = {}\n","result['predResult'] = predResult       # Prediction values for the training, validation, and testing sets\n","result['perM'] = perM                   # Loss values of training and validation during model training\n","result['perf'] = perf                   # Prediction performance metrics\n","result['winningModel'] = winningModel   # Model with the minimum validation loss\n","result['batch_size'] = batch_size       # Batch size used in model training\n","\n","# Save prediction performance and all data and results\n","perf.to_csv(result_dir + '/Prediction_Performance.txt', header=True, index=True, sep='\\t', line_terminator='\\r\\n')\n","output = open(result_dir + '/Result.pkl', 'wb')\n","cp.dump(res, output)\n","cp.dump(ccl, output)\n","cp.dump(drug, output)\n","cp.dump(result, output)\n","cp.dump(sampleID, output)\n","cp.dump(para, output)\n","output.close()"],"metadata":{"id":"FS0FR6xeFz3c"},"execution_count":null,"outputs":[]}]}